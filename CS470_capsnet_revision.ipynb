{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS470_capsnet_revision.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ehgus/CS420CompilerDesign/blob/master/CS470_capsnet_revision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__cjCHwZkomW",
        "colab_type": "text"
      },
      "source": [
        "# Capsnet\n",
        "- dataset: mnist\n",
        "- model: capsnet+FCnet\n",
        "\n",
        "Future usage: decomposition of overlaped images\n",
        "\n",
        "Future task\n",
        "1. Visualize the model\n",
        "2. Check each layers are functioning as intended(freezing some layer, scheduler, etc)\n",
        "3. stabilize the code on gpu environment\n",
        "4. check squeeze function also works well on batch==1 case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdPrF_Ztr4Zy",
        "colab_type": "text"
      },
      "source": [
        "# 0. Module importing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxbAYTcyxUZ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e5dd19fc-a59b-4f1e-eb55-e8672d911925"
      },
      "source": [
        "#For path setting and image loading\n",
        "import os\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "#For implementing train part\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.init import constant_\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, lr_scheduler\n",
        "\n",
        "\n",
        "#For image creation\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "from PIL import Image\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device setting: {device}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device setting: cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du464-D1rtgg",
        "colab_type": "text"
      },
      "source": [
        "# 1. Gdrivce mounting\n",
        "Mount google drive to the program. \n",
        "\n",
        "This might change your drive. It's recommended to executing it <b>locally</b>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2VZgrADtiBc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "549d7951-56a6-4194-f1c7-668f64cd989f"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "gdrive_root = '/gdrive/My Drive'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSCfJD09o_H1",
        "colab_type": "text"
      },
      "source": [
        "# 2. Data loading module\n",
        "Module for loading mnist dataset with batch parsing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDl_IVvkxanl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_mnist(path, batch_size=100, shift_pixels=2):\n",
        "    \"\"\"\n",
        "    Construct dataloaders for training and test data. Data augmentation is also done here.\n",
        "    :param path: file path of the dataset\n",
        "    :param batch_size: batch size\n",
        "    :param shift_pixels: maximum number of pixels to shift in each direction\n",
        "    \n",
        "    :return: train_loader, test_loader\n",
        "    \"\"\"\n",
        "    \n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "          datasets.MNIST(path, train=True,download=True, transform=transforms.Compose([transforms.RandomCrop(size=28, padding=shift_pixels), transforms.ToTensor()])), batch_size=batch_size, shuffle=True, **kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "          datasets.MNIST(path, train=False,download=True, transform=transforms.ToTensor()), batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwgS_4JTrY1n",
        "colab_type": "text"
      },
      "source": [
        "# 3.1. CapsNet Network layer modules\n",
        "modules of capsnet architecture\n",
        "\n",
        "<b>def</b> squash\n",
        "- Vector is squashed with lengh 0~1.\n",
        "\n",
        "<b>class</b> DenseCapsule\n",
        "- Core part with dynamic routing\n",
        "\n",
        "<b>class</b> PrimaryCapsule\n",
        "- Vector creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZzUocfjyOoI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def squash(inputs, axis=-1):\n",
        "    \"\"\"\n",
        "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
        "    :param inputs: vectors to be squashed\n",
        "    :param axis: the axis to squash. negative axis means axis=axis+dimension-1.\n",
        "\n",
        "    :return: a Tensor with same size as inputs\n",
        "    \"\"\"\n",
        "    norm = torch.norm(inputs, p=2, dim=axis, keepdim=True)\n",
        "    scale = norm**2 / (1 + norm**2) / (norm + 1e-8)\n",
        "    return scale * inputs\n",
        "\n",
        "\n",
        "class DenseCapsule(nn.Module):\n",
        "    \"\"\"\n",
        "    The dense capsule layer. It is similar to Dense (FC) layer. Dense layer has `in_num` inputs, each is a scalar, the\n",
        "    output of the neuron from the former layer, and it has `out_num` output neurons. DenseCapsule just expands the\n",
        "    output of the neuron from scalar to vector. So its input size = [None, in_num_caps, in_dim_caps] and output size = \\\n",
        "    [None, out_num_caps, out_dim_caps]. For Dense Layer, in_dim_caps = out_dim_caps = 1.\n",
        "    :param in_num_caps: number of cpasules inputted to this layer\n",
        "    :param in_dim_caps: dimension of input capsules\n",
        "    :param out_num_caps: number of capsules outputted from this layer\n",
        "    :param out_dim_caps: dimension of output capsules\n",
        "    :param routings: number of iterations for the routing algorithm\n",
        "    \"\"\"\n",
        "    def __init__(self,in_num_caps, in_dim_caps, out_num_caps, out_dim_caps, routings=3):\n",
        "        super(DenseCapsule, self).__init__()\n",
        "        self.in_num_caps = in_num_caps\n",
        "        self.in_dim_caps = in_dim_caps\n",
        "        self.out_num_caps = out_num_caps\n",
        "        self.out_dim_caps = out_dim_caps\n",
        "        self.routings = routings\n",
        "        self.weight = nn.Parameter( 0.01*torch.randn(out_num_caps, in_num_caps, out_dim_caps, in_dim_caps))\n",
        "\n",
        "    def forward(self, x):\n",
        "        #assertion and initalization\n",
        "        assert self.routings >=1, \"The 'routings' should be > 0.\"\n",
        "        '''\n",
        "        step1: Affine transforatmion\n",
        "        '''\n",
        "        # weight.size  =      [ out_num_caps, in_num_caps, out_dim_caps, in_dim_caps]\n",
        "        # x.size       =[batch, 1           , in_num_caps,  in_dim_caps, 1          ]\n",
        "        #rst:x_hat.size=[batch, out_num_caps, in_num_caps, out_dim_caps]\n",
        "\n",
        "        # torch.matmul: [out_dim_caps, in_dim_caps] x [in_dim_caps, 1] -> [out_dim_caps, 1]\n",
        "        # squeeze     : [batch,out_num_caps, in_num_caps, out_dim_caps, 1] -> [batch,out_num_caps, in_num_caps, out_dim_caps]\n",
        "\n",
        "        x_hat = torch.squeeze(torch.matmul(self.weight, x[:, None, :, :, None]),dim=-1)\n",
        "        b = torch.zeros(x.size(0), self.out_num_caps, self.in_num_caps).to(device)\n",
        "        '''\n",
        "        step2: dynamic routing\n",
        "        '''\n",
        "        # For iteration, use detached matrix so that no gradients flow on this path.\n",
        "        # In backward, no gradient can flow from `x_hat_detached`.\n",
        "        x_hat_detached = x_hat.detach()\n",
        "        \n",
        "        for i in range(self.routings-1):\n",
        "            # c.size = [batch, out_num_caps, in_num_caps]\n",
        "            c = F.softmax(b, dim=-1)\n",
        "            \n",
        "            # c.size expanded to [batch, out_num_caps, in_num_caps, 1           ]\n",
        "            # x_hat.size     =   [batch, out_num_caps, in_num_caps, out_dim_caps]\n",
        "            # => outputs.size=   [batch, out_num_caps, 1          , out_dim_caps]     \n",
        "            outputs = squash(torch.sum(c[:, :, :, None] * x_hat_detached,dim=-2,keepdim=True))\n",
        "                \n",
        "            # outputs.size       =[batch, out_num_caps, None,      , out_dim_caps]\n",
        "            # x_hat_detached.size=[batch, out_num_caps, in_num_caps, out_dim_caps]\n",
        "            # => b.size          =[batch, out_num_caps, in_num_caps]\n",
        "            b =b+ torch.sum(outputs * x_hat_detached, dim=-1)\n",
        "\n",
        "        c = F.softmax(b, dim=-1)\n",
        "        # => outputs.size=   [batch, out_num_caps, out_dim_caps]\n",
        "        outputs=squash(torch.sum(c[:, :, :, None] * x_hat,dim=-2))\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class PrimaryCapsule(nn.Module):\n",
        "    \"\"\"\n",
        "    Apply Conv2D with `out_channels` and then reshape to get capsules\n",
        "    :param in_channels: input channels\n",
        "    :param out_channels: output channels\n",
        "    :param dim_caps: dimension of capsule\n",
        "    :param kernel_size: kernel size\n",
        "    :return: output tensor, size=[batch, num_caps, dim_caps]\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, dim_caps, kernel_size, stride=1, padding=0):\n",
        "        super(PrimaryCapsule, self).__init__()\n",
        "        self.dim_caps = dim_caps\n",
        "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,groups=in_channels//dim_caps,bias=False)\n",
        "        # Set requires_grad to False\n",
        "        #conv2 initialization: no routing algorithm\n",
        "        #Each values of conv filters are supposed to be calculated by softmax function, setting to 1 to all parameter doesn't matter. Denominator than wil be movoed on previous conv network.\n",
        "        # self.conv2d.weight=torch.ones_like(self.conv2d.weight)\n",
        "        self.conv2d.weight=nn.Parameter(torch.ones_like(self.conv2d.weight),requires_grad=False)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        # outputs.size= [batch, in_num_caps, in_dim_caps]\n",
        "        outputs = squash(self.conv2d(x).view(x.size(0), -1, self.dim_caps))\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4etjyJoyRFU",
        "colab_type": "text"
      },
      "source": [
        "#3.2. CapsNet Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmtcrkayyYJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CapsuleNet(nn.Module):\n",
        "    \"\"\"\n",
        "    :param input_size: data size = [channels, width, height]\n",
        "    :param classes: number of classes\n",
        "    :param routings: number of routing iterations\n",
        "    Shape:\n",
        "        - Input: (batch, channels, width, height), optional (batch, classes) .\n",
        "        - Output:((batch, classes), (batch, channels, width, height))\n",
        "    \"\"\"\n",
        "    def __init__(self ,input_size, classes,num_labels,routings):\n",
        "        super(CapsuleNet, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.classes = classes\n",
        "        self.routings = routings\n",
        "\n",
        "        #encoder network\n",
        "        self.encoder=nn.Sequential(\n",
        "            nn.Conv2d(input_size[0], 256, kernel_size=9, stride=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            PrimaryCapsule(256, 256, 8, kernel_size=9, stride=2, padding=0),\n",
        "            DenseCapsule(in_num_caps=32*6*6, in_dim_caps=8, out_num_caps=classes, out_dim_caps=16, routings=routings)\n",
        "        )\n",
        "        # Decoder network.\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(16*classes, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, input_size[0] * input_size[1] * input_size[2]),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        x = self.encoder(x)\n",
        "        length = x.norm(p=2, dim=-1)\n",
        "        # during testing, no label given. create one-hot coding using 'length'\n",
        "        if y is None: \n",
        "            index = length.max(dim=1)[1] #return named indices\n",
        "            y = torch.zeros(length.size()).scatter_(1, index.view(-1, 1).cpu().data, 1.).to(device)\n",
        "        reconstruction = self.decoder((x * y[:, :, None]).view(x.size(0), -1))\n",
        "\n",
        "        return length, reconstruction.view(-1, *self.input_size)\n",
        "\n",
        "def caps_loss(y_true, y_pred, x, x_recon, lam_recon):\n",
        "    \"\"\"\n",
        "    Capsule loss = Margin loss + lam_recon * reconstruction loss.\n",
        "    :param y_true: true labels, one-hot coding, size=[batch, classes]\n",
        "    :param y_pred: predicted labels by CapsNet, size=[batch, classes]\n",
        "    :param x: input data, size=[batch, channels, width, height]\n",
        "    :param x_recon: reconstructed data, size is same as `x`\n",
        "    :param lam_recon: coefficient for reconstruction loss\n",
        "    :return: variable contains a scalar loss value.\n",
        "    \"\"\"\n",
        "    L = y_true * torch.clamp(0.9 - y_pred, min=0.) ** 2 + \\\n",
        "        0.5 * (1 - y_true) * torch.clamp(y_pred - 0.1, min=0.) ** 2\n",
        "    L_margin = L.sum(dim=1).mean()\n",
        "\n",
        "    L_recon = nn.MSELoss()(x_recon, x)\n",
        "    return L_margin + lam_recon * L_recon\n",
        "\n",
        "\n",
        "def show_reconstruction(model, test_loader, n_images, args):\n",
        "    import matplotlib.pyplot as plt\n",
        "    from utils import combine_images\n",
        "    from PIL import Image\n",
        "    import numpy as np\n",
        "\n",
        "    model.eval()\n",
        "    for x, _ in test_loader:\n",
        "        x = x[:min(n_images, x.size(0))].to(device)\n",
        "        _, x_recon = model(x)\n",
        "        data = np.concatenate([x.data, x_recon.data])\n",
        "        img = combine_images(np.transpose(data, [0, 2, 3, 1]))\n",
        "        image = img * 255\n",
        "        Image.fromarray(image.astype(np.uint8)).save(args['save_dir'] + \"/real_and_recon.png\")\n",
        "        print()\n",
        "        print('Reconstructed images are saved to %s/real_and_recon.png' % args['save_dir'])\n",
        "        print('-' * 70)\n",
        "        plt.imshow(plt.imread(args['save_dir'] + \"/real_and_recon.png\", ))\n",
        "        plt.show()\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oigAjJKYspPD",
        "colab_type": "text"
      },
      "source": [
        "#4. model initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2qPyEnnyb9c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "75eb39f6-3b71-4a8a-f7cf-39a0700e7fe1"
      },
      "source": [
        "# setting the hyper parameters\n",
        "args={'epochs': 50,\n",
        "      'batch_size':100,\n",
        "      'lr':0.001,                         #Initial learning rate\n",
        "      'lr_decay':0.9,                     #The value multiplied by lr at each epoch. Set a larger value for larger epochs\n",
        "      'lam_recon': 0.0005 * 784,          #The coefficient for the loss of decoder\n",
        "      'data_dir':'/my_data',                #Directory of data. If no data, use \\'--download\\' flag to download it\n",
        "      'save_dir':'/my_data'\n",
        "      }\n",
        "\n",
        "model = CapsuleNet(input_size=[1, 28, 28], classes=10,num_labels=10, routings=3)\n",
        "model = model.to(device)\n",
        "#optimizer setting\n",
        "optimizer = Adam(model.parameters(), lr=args['lr'])\n",
        "lr_decay = lr_scheduler.ExponentialLR(optimizer, gamma=args['lr_decay'])\n",
        "\n",
        "# Print your neural network structure\n",
        "print(model)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CapsuleNet(\n",
            "  (encoder): Sequential(\n",
            "    (0): Conv2d(1, 256, kernel_size=(9, 9), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): PrimaryCapsule(\n",
            "      (conv2d): Conv2d(256, 256, kernel_size=(9, 9), stride=(2, 2), groups=32, bias=False)\n",
            "    )\n",
            "    (3): DenseCapsule()\n",
            "  )\n",
            "  (decoder): Sequential(\n",
            "    (0): Linear(in_features=160, out_features=512, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=512, out_features=1024, bias=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Linear(in_features=1024, out_features=784, bias=True)\n",
            "    (5): Sigmoid()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpvYyZpGsYEu",
        "colab_type": "text"
      },
      "source": [
        "# 5. Loading pre-trained model if exist\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cR18S9JtqkF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "02b06a14-97d4-425c-bdaf-ad9b2516a1a6"
      },
      "source": [
        "#check point \n",
        "ckpt_dir = os.path.join(gdrive_root, 'checkpoints')\n",
        "if not os.path.exists(ckpt_dir):\n",
        "  os.makedirs(ckpt_dir)\n",
        "  \n",
        "best_acc = 0.\n",
        "ckpt_path = os.path.join(ckpt_dir, 'lastest.pt')\n",
        "if os.path.exists(ckpt_path):\n",
        "  ckpt = torch.load(ckpt_path)\n",
        "  try:\n",
        "    model.load_state_dict(ckpt['model'])\n",
        "    optimizer.load_state_dict(ckpt['optimizer'])\n",
        "    best_acc = ckpt['best_acc']\n",
        "  except RuntimeError as e:\n",
        "      print('wrong checkpoint')\n",
        "  else:    \n",
        "    print('checkpoint is loaded !')\n",
        "    print('current best accuracy : %.2f' % best_acc)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint is loaded !\n",
            "current best accuracy : 0.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVdEiVedrg26",
        "colab_type": "text"
      },
      "source": [
        "# 6. Training and testing model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms1GlBWurpcx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, test_loader, args):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for x, y in test_loader:\n",
        "\n",
        "        # change to one-hot coding\n",
        "        # convert input data to device\n",
        "        y = torch.zeros(y.size(0), 10).scatter(1, y.view(-1, 1), 1.)\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        #test\n",
        "        y_pred, x_recon = model(x)\n",
        "        # sum up batch loss\n",
        "        test_loss += caps_loss(y, y_pred, x, x_recon, args['lam_recon']).data * x.size(0)  \n",
        "        y_pred = y_pred.data.max(1)[1]\n",
        "        y_true = y.data.max(1)[1]\n",
        "        correct += torch.sum(y_pred.eq(y_true)) \n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    return test_loss, correct / len(test_loader.dataset)\n",
        "\n",
        "\n",
        "def train(model, train_loader, test_loader, args, best_acc):\n",
        "    \"\"\"\n",
        "    Training a CapsuleNet\n",
        "    :param model: the CapsuleNet model\n",
        "    :param train_loader: torch.utils.data.DataLoader for training data\n",
        "    :param test_loader: torch.utils.data.DataLoader for test data\n",
        "    :param args: arguments\n",
        "    :return: The trained model\n",
        "    \"\"\"\n",
        "    print('Begin Training' + '-'*70)\n",
        "    from time import time\n",
        "    \n",
        "    t0 = time()\n",
        "\n",
        "    \n",
        "    for epoch in range(args['epochs']):\n",
        "        #train phase    \n",
        "        model.train()\n",
        "\n",
        "        # decrease the learning rate by multiplying a factor `gamma`\n",
        "        lr_decay.step()  \n",
        "        ti = time()\n",
        "        \n",
        "        training_loss = 0.0\n",
        "        for x, y in train_loader:\n",
        "            \n",
        "            # change to one-hot coding\n",
        "            # convert input data to GPU Variable\n",
        "            y = torch.zeros(y.size(0), 10).scatter_(1, y.view(-1, 1), 1.)\n",
        "\n",
        "            # 수정 전: x, y = Variable(x.cuda()), Variable(y.cuda())\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # set gradients of optimizer to zero\n",
        "            optimizer.zero_grad()  \n",
        "\n",
        "            # forward\n",
        "            y_pred, x_recon = model(x, y)  \n",
        "            \n",
        "            #compute loss\n",
        "            loss = caps_loss(y, y_pred, x, x_recon, args['lam_recon'])\n",
        "\n",
        "            # backward\n",
        "            loss.backward() \n",
        "\n",
        "            #수정 전 loss.data[0] * x.size(0)\n",
        "            training_loss += loss.data * x.size(0)  # record the batch loss\n",
        "            optimizer.step()  # update the trainable parameters with computed gradients\n",
        "\n",
        "        # compute validation loss and acc\n",
        "        test_loss, test_acc = test(model, test_loader, args)\n",
        "        \n",
        "        print((f\"==> Epoch {epoch}: \"\n",
        "               f\" training loss={training_loss / len(train_loader.dataset):.5f}, \"\n",
        "               f\" test loss={test_loss:.5f}, \"\n",
        "               f\" test acc={test_acc:.4f}, \"\n",
        "               f\" iteration time={time()-ti:.1f}s\"))\n",
        "        if test_acc >= best_acc:  # update best validation acc and save model\n",
        "            best_acc = test_acc\n",
        "            # Note: optimizer also has states ! don't forget to save them as well.\n",
        "            ckpt = {'model':model.state_dict(),\n",
        "                    'optimizer':optimizer.state_dict(),\n",
        "                    'best_acc':best_acc}\n",
        "            torch.save(ckpt, ckpt_path)\n",
        "            print('checkpoint is saved !')\n",
        "    print(\"Total time = %ds\" % (time() - t0))\n",
        "    print('End Training' + '-' * 70)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtSvs43Z_JFe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        },
        "outputId": "69c03b1e-b092-4fe9-db97-56e03410aeff"
      },
      "source": [
        "##problem: util module cannot be imported: use other methods\n",
        "##parser makes error: redisign it\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if True:\n",
        "    # load data\n",
        "    train_loader, test_loader = load_mnist(path=gdrive_root+args['data_dir'], batch_size=args['batch_size'])\n",
        "    # train model\n",
        "    train(model, train_loader, test_loader, args, best_acc)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Begin Training----------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==> Epoch 0:  training loss=0.03913,  test loss=0.03168,  test acc=0.0000,  iteration time=938.4s\n",
            "checkpoint is saved !\n",
            "==> Epoch 1:  training loss=0.03803,  test loss=0.03147,  test acc=0.0000,  iteration time=913.2s\n",
            "checkpoint is saved !\n",
            "==> Epoch 2:  training loss=0.03681,  test loss=0.03041,  test acc=0.0000,  iteration time=860.0s\n",
            "checkpoint is saved !\n",
            "==> Epoch 3:  training loss=0.03608,  test loss=0.03020,  test acc=0.0000,  iteration time=824.9s\n",
            "checkpoint is saved !\n",
            "==> Epoch 4:  training loss=0.03529,  test loss=0.02993,  test acc=0.0000,  iteration time=825.4s\n",
            "checkpoint is saved !\n",
            "==> Epoch 5:  training loss=0.03482,  test loss=0.02903,  test acc=0.0000,  iteration time=818.5s\n",
            "checkpoint is saved !\n",
            "==> Epoch 6:  training loss=0.03375,  test loss=0.02967,  test acc=0.0000,  iteration time=828.6s\n",
            "checkpoint is saved !\n",
            "==> Epoch 7:  training loss=0.03335,  test loss=0.03013,  test acc=0.0000,  iteration time=826.9s\n",
            "checkpoint is saved !\n",
            "==> Epoch 8:  training loss=0.03269,  test loss=0.02852,  test acc=0.0000,  iteration time=826.4s\n",
            "checkpoint is saved !\n",
            "==> Epoch 9:  training loss=0.03214,  test loss=0.02889,  test acc=0.0000,  iteration time=829.3s\n",
            "checkpoint is saved !\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-dc26e91d6468>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgdrive_root\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_dir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-381b9acdaa9a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, args, best_acc)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_recon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m#compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-52e2e101ef31>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# during testing, no label given. create one-hot coding using 'length'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-23bc8bad2801>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;31m# x_hat.size     =   [batch, out_num_caps, in_num_caps, out_dim_caps]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;31m# => outputs.size=   [batch, out_num_caps, 1          , out_dim_caps]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msquash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_hat_detached\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;31m# outputs.size       =[batch, out_num_caps, None,      , out_dim_caps]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsjY5ynxstXU",
        "colab_type": "text"
      },
      "source": [
        "# 7. Plotting the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksX1GnfauiQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_reconstruction(model, test_loader, n_images):\n",
        "\n",
        "    model.eval()\n",
        "    for x, _ in test_loader:\n",
        "        #수정 전: x = Variable(x[:min(n_images, x.size(0))].cuda(), volatile=True)\n",
        "        x = x[:min(n_images, x.size(0))].to(device)\n",
        "        _, x_recon = model(x)\n",
        "        data = np.concatenate([x.data, x_recon.data])\n",
        "        img = combine_images(np.transpose(data, [0, 2, 3, 1]))\n",
        "        image = img * 255\n",
        "        Image.fromarray(image.astype(np.uint8)).save(gdrive+ \"my_data/real_and_recon.png\")\n",
        "        print()\n",
        "        print('Reconstructed images are saved to my_data/real_and_recon.png')\n",
        "        print('-' * 70)\n",
        "        plt.imshow(plt.imread(gdrive + \"my_data/real_and_recon.png\", ))\n",
        "        plt.show()\n",
        "        break\n",
        "\n",
        "def combine_images(generated_images):\n",
        "    num = generated_images.shape[0]\n",
        "    width = int(math.sqrt(num))\n",
        "    height = int(math.ceil(float(num)/width))\n",
        "    shape = generated_images.shape[1:3]\n",
        "    image = np.zeros((height*shape[0], width*shape[1]),\n",
        "                     dtype=generated_images.dtype)\n",
        "    for index, img in enumerate(generated_images):\n",
        "        i = int(index/width)\n",
        "        j = index % width\n",
        "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
        "            img[:, :, 0]\n",
        "    return image\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    plot_log('result/log.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMxINtEcsyc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if False:\n",
        "  test_loss, test_acc = test(model, test_loader,args)\n",
        "  print(f'test acc = {test_acc:.4f}, test loss = {test_loss:.5f}')\n",
        "  show_reconstruction(model, test_loader, 50)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}